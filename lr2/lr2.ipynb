{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv('grupo-bimbo-inventory-demand/train.csv')\n",
    "test = pd.read_csv('grupo-bimbo-inventory-demand/test.csv')\n",
    "\n",
    "print('Train and Test Read')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "train['target'] = train['Demanda_uni_equil']\n",
    "train.drop(['Demanda_uni_equil'],axis=1, inplace = True)\n",
    "\n",
    "train['tst'] = 0\n",
    "test['tst'] = 1\n",
    "\n",
    "data = pd.concat([train,test], axis=0, copy=True)\n",
    "\n",
    "print('Train and Test Concat')\n",
    "del train\n",
    "del test\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    lag = 'Lag' + str(i)\n",
    "    print('Lag:',lag)\n",
    "\n",
    "    data1 = data[['Semana','Cliente_ID','Producto_ID','target']]\n",
    "    data1.loc[:,'Semana'] = data1['Semana'] +i\n",
    "    data1 = data1.groupby(['Semana','Cliente_ID','Producto_ID']).mean()\n",
    "    data1 = data1.reset_index()\n",
    "    data1.rename(columns={'target': lag}, inplace=True)\n",
    "    data = pd.merge(data,data1,\n",
    "                    how='left',\n",
    "                    left_on=['Semana','Cliente_ID','Producto_ID'],\n",
    "                    right_on=['Semana','Cliente_ID','Producto_ID'],\n",
    "                    left_index=False, right_index=False, sort=True,\n",
    "                    suffixes=('_x', '_y'), copy=False, )\n",
    "    del data1\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "data['TotalLags'] = data['Lag1'] + data['Lag2']+ data['Lag3']+ data['Lag4']+ data['Lag5']\n",
    "\n",
    "data=data[data['Semana']>8]  # NOW I WORK WITH WEEKS 9,10,11\n",
    "\n",
    "nAgencia = pd.DataFrame(data.groupby(['Agencia_ID','Semana'])['target'].count())\n",
    "nAgencia = nAgencia.reset_index()\n",
    "nAgencia.rename(columns={'target': 'nAgencia'}, inplace=True)\n",
    "nAgencia = pd.DataFrame(nAgencia.groupby(['Agencia_ID'])['nAgencia'].mean())\n",
    "nAgencia = nAgencia.reset_index()\n",
    "\n",
    "\n",
    "data = pd.merge(data, nAgencia,\n",
    "                how='left',\n",
    "                left_on=['Agencia_ID'],\n",
    "                right_on=['Agencia_ID'],\n",
    "                left_index=False, right_index=False, sort=True,\n",
    "                suffixes=('_x', '_y'), copy=False)\n",
    "\n",
    "del nAgencia\n",
    "gc.collect()\n",
    "print('merge completo nAgencia')\n",
    "print(data.shape[0])\n",
    "\n",
    "nRuta_SAK = pd.DataFrame(data.groupby(['Ruta_SAK','Semana'])['target'].count())\n",
    "nRuta_SAK = nRuta_SAK.reset_index()\n",
    "nRuta_SAK.rename(columns={'target': 'nRuta_SAK'}, inplace=True)\n",
    "nRuta_SAK = pd.DataFrame(nRuta_SAK.groupby(['Ruta_SAK'])['nRuta_SAK'].mean())\n",
    "nRuta_SAK = nRuta_SAK.reset_index()\n",
    "\n",
    "\n",
    "data = pd.merge(data, nRuta_SAK,\n",
    "                how='left',\n",
    "                left_on=['Ruta_SAK'],\n",
    "                right_on=['Ruta_SAK'],\n",
    "                left_index=False, right_index=False, sort=True,\n",
    "                suffixes=('_x', '_y'), copy=False)\n",
    "\n",
    "del nRuta_SAK\n",
    "gc.collect()\n",
    "print('merge completo nRuta_SAK')\n",
    "print(data.shape[0])\n",
    "\n",
    "nCliente_ID = pd.DataFrame(data.groupby(['Cliente_ID','Semana'])['target'].count())\n",
    "nCliente_ID = nCliente_ID.reset_index()\n",
    "nCliente_ID.rename(columns={'target': 'nCliente_ID'}, inplace=True)\n",
    "nCliente_ID = pd.DataFrame(nCliente_ID.groupby(['Cliente_ID'])['nCliente_ID'].mean())\n",
    "nCliente_ID = nCliente_ID.reset_index()\n",
    "\n",
    "\n",
    "data = pd.merge(data, nCliente_ID,\n",
    "                how='left',\n",
    "                left_on=['Cliente_ID'],\n",
    "                right_on=['Cliente_ID'],\n",
    "                left_index=False, right_index=False, sort=True,\n",
    "                suffixes=('_x', '_y'), copy=False)\n",
    "\n",
    "del nCliente_ID\n",
    "gc.collect()\n",
    "print('merge completo nCliente_ID')\n",
    "print(data.shape[0])\n",
    "\n",
    "nProducto_ID = pd.DataFrame(data.groupby(['Producto_ID','Semana'])['target'].count())\n",
    "nProducto_ID = nProducto_ID.reset_index()\n",
    "nProducto_ID.rename(columns={'target': 'nProducto_ID'}, inplace=True)\n",
    "nProducto_ID = pd.DataFrame(nProducto_ID.groupby(['Producto_ID'])['nProducto_ID'].mean())\n",
    "nProducto_ID = nProducto_ID.reset_index()\n",
    "\n",
    "\n",
    "data = pd.merge(data, nProducto_ID,\n",
    "                how='left',\n",
    "                left_on=['Producto_ID'],\n",
    "                right_on=['Producto_ID'],\n",
    "                left_index=False, right_index=False, sort=True,\n",
    "                suffixes=('_x', '_y'), copy=False)\n",
    "\n",
    "del nProducto_ID\n",
    "gc.collect()\n",
    "print('merge completo nProducto_ID')\n",
    "print(data.shape[0])\n",
    "\n",
    "data.replace(np.nan,0, inplace=True)\n",
    "\n",
    "train = data[data['tst']==0]\n",
    "predict = data[data['tst']==1]\n",
    "\n",
    "train['target'] = np.log(train['target'] + 1)\n",
    "#train2 = train.sample(n=1000000)   <-- another possible reduction of data for fast testing\n",
    "train2 = train\n",
    "y = train['target']\n",
    "X = train[[  'Agencia_ID','Canal_ID','Cliente_ID','Producto_ID','Ruta_SAK',\n",
    "             'Lag1','Lag2','Lag3','Lag4','Lag5','TotalLags',\n",
    "             'nAgencia','nRuta_SAK','nCliente_ID','nProducto_ID']]\n",
    "\n",
    "print(X.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X.to_csv('x.csv',index=False)\n",
    "# y.to_csv('y.csv',index=False)\n",
    "train.to_csv('data.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sns.palplot(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10304625, 15) (104088, 15)\n"
     ]
    }
   ],
   "source": [
    "X=pd.read_csv('x.csv')\n",
    "y=pd.read_csv('y.csv')\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=1729)\n",
    "print(X_train.shape, X_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "xlf = xgb.XGBRegressor(max_depth=10,\n",
    "                       learning_rate=0.1,\n",
    "                       objective=\"reg:squarederror\",\n",
    "                       nthread=-1,\n",
    "                       gamma=0,\n",
    "                       seed=1440,\n",
    "                       missing=1,\n",
    "                       eval_metric='rmsle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:32:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-rmsle:0.51802\n",
      "[1]\tvalidation_0-rmsle:0.45946\n",
      "[2]\tvalidation_0-rmsle:0.41249\n",
      "[3]\tvalidation_0-rmsle:0.37411\n",
      "[4]\tvalidation_0-rmsle:0.34310\n",
      "[5]\tvalidation_0-rmsle:0.31783\n",
      "[6]\tvalidation_0-rmsle:0.29627\n",
      "[7]\tvalidation_0-rmsle:0.27899\n",
      "[8]\tvalidation_0-rmsle:0.26432\n",
      "[9]\tvalidation_0-rmsle:0.25294\n"
     ]
    },
    {
     "data": {
      "text/plain": "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric='rmsle', gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n             missing=1, monotone_constraints='()', n_estimators=10, n_jobs=-1,\n             nthread=-1, num_parallel_tree=1, predictor='auto',\n             random_state=1440, reg_alpha=0, ...)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlf.fit(X_train, y_train, verbose = True, eval_set = [(X_test, y_test)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSLE XDBoost  0.25294002423995865\n"
     ]
    }
   ],
   "source": [
    "# calculate the auc score\n",
    "preds = xlf.predict(X_test)\n",
    "\n",
    "print('Validation RMSLE XDBoost ', mean_squared_log_error(y_test,preds, squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\torch\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.627009\n",
      "[2]\tvalid_0's l2: 0.557831\n",
      "[3]\tvalid_0's l2: 0.501662\n",
      "[4]\tvalid_0's l2: 0.455631\n",
      "[5]\tvalid_0's l2: 0.418407\n",
      "[6]\tvalid_0's l2: 0.38801\n",
      "[7]\tvalid_0's l2: 0.362009\n",
      "[8]\tvalid_0's l2: 0.340101\n",
      "[9]\tvalid_0's l2: 0.32279\n",
      "[10]\tvalid_0's l2: 0.307942\n",
      "[11]\tvalid_0's l2: 0.296124\n",
      "[12]\tvalid_0's l2: 0.286203\n",
      "[13]\tvalid_0's l2: 0.277763\n",
      "[14]\tvalid_0's l2: 0.270854\n",
      "[15]\tvalid_0's l2: 0.264717\n",
      "[16]\tvalid_0's l2: 0.259782\n",
      "[17]\tvalid_0's l2: 0.255866\n",
      "[18]\tvalid_0's l2: 0.251835\n",
      "[19]\tvalid_0's l2: 0.248745\n",
      "[20]\tvalid_0's l2: 0.245626\n",
      "[21]\tvalid_0's l2: 0.243485\n",
      "[22]\tvalid_0's l2: 0.241481\n",
      "[23]\tvalid_0's l2: 0.23939\n",
      "[24]\tvalid_0's l2: 0.237606\n",
      "[25]\tvalid_0's l2: 0.236359\n",
      "[26]\tvalid_0's l2: 0.235169\n",
      "[27]\tvalid_0's l2: 0.233922\n",
      "[28]\tvalid_0's l2: 0.232997\n",
      "[29]\tvalid_0's l2: 0.232219\n",
      "[30]\tvalid_0's l2: 0.231336\n",
      "[31]\tvalid_0's l2: 0.230564\n",
      "[32]\tvalid_0's l2: 0.229731\n",
      "[33]\tvalid_0's l2: 0.22895\n",
      "[34]\tvalid_0's l2: 0.228189\n",
      "[35]\tvalid_0's l2: 0.227583\n",
      "[36]\tvalid_0's l2: 0.226809\n",
      "[37]\tvalid_0's l2: 0.226053\n",
      "[38]\tvalid_0's l2: 0.225339\n",
      "[39]\tvalid_0's l2: 0.224873\n",
      "[40]\tvalid_0's l2: 0.224372\n",
      "[41]\tvalid_0's l2: 0.223604\n",
      "[42]\tvalid_0's l2: 0.222838\n",
      "[43]\tvalid_0's l2: 0.222415\n",
      "[44]\tvalid_0's l2: 0.222104\n",
      "[45]\tvalid_0's l2: 0.221628\n",
      "[46]\tvalid_0's l2: 0.221172\n",
      "[47]\tvalid_0's l2: 0.220846\n",
      "[48]\tvalid_0's l2: 0.220513\n",
      "[49]\tvalid_0's l2: 0.220232\n",
      "[50]\tvalid_0's l2: 0.219932\n",
      "[51]\tvalid_0's l2: 0.219503\n",
      "[52]\tvalid_0's l2: 0.219094\n",
      "[53]\tvalid_0's l2: 0.218845\n",
      "[54]\tvalid_0's l2: 0.218485\n",
      "[55]\tvalid_0's l2: 0.218096\n",
      "[56]\tvalid_0's l2: 0.217916\n",
      "[57]\tvalid_0's l2: 0.217738\n",
      "[58]\tvalid_0's l2: 0.217504\n",
      "[59]\tvalid_0's l2: 0.217319\n",
      "[60]\tvalid_0's l2: 0.217137\n",
      "[61]\tvalid_0's l2: 0.216962\n",
      "[62]\tvalid_0's l2: 0.21672\n",
      "[63]\tvalid_0's l2: 0.216538\n",
      "[64]\tvalid_0's l2: 0.216313\n",
      "[65]\tvalid_0's l2: 0.216237\n",
      "[66]\tvalid_0's l2: 0.216024\n",
      "[67]\tvalid_0's l2: 0.215771\n",
      "[68]\tvalid_0's l2: 0.215612\n",
      "[69]\tvalid_0's l2: 0.215421\n",
      "[70]\tvalid_0's l2: 0.215294\n",
      "[71]\tvalid_0's l2: 0.215081\n",
      "[72]\tvalid_0's l2: 0.214994\n",
      "[73]\tvalid_0's l2: 0.214657\n",
      "[74]\tvalid_0's l2: 0.214485\n",
      "[75]\tvalid_0's l2: 0.214359\n",
      "[76]\tvalid_0's l2: 0.214218\n",
      "[77]\tvalid_0's l2: 0.214101\n",
      "[78]\tvalid_0's l2: 0.213874\n",
      "[79]\tvalid_0's l2: 0.21376\n",
      "[80]\tvalid_0's l2: 0.213628\n",
      "[81]\tvalid_0's l2: 0.213406\n",
      "[82]\tvalid_0's l2: 0.213337\n",
      "[83]\tvalid_0's l2: 0.213189\n",
      "[84]\tvalid_0's l2: 0.213116\n",
      "[85]\tvalid_0's l2: 0.213037\n",
      "[86]\tvalid_0's l2: 0.212963\n",
      "[87]\tvalid_0's l2: 0.212927\n",
      "[88]\tvalid_0's l2: 0.212834\n",
      "[89]\tvalid_0's l2: 0.2127\n",
      "[90]\tvalid_0's l2: 0.212639\n",
      "[91]\tvalid_0's l2: 0.212604\n",
      "[92]\tvalid_0's l2: 0.212454\n",
      "[93]\tvalid_0's l2: 0.212334\n",
      "[94]\tvalid_0's l2: 0.212265\n",
      "[95]\tvalid_0's l2: 0.212171\n",
      "[96]\tvalid_0's l2: 0.212026\n",
      "[97]\tvalid_0's l2: 0.211968\n",
      "[98]\tvalid_0's l2: 0.211941\n",
      "[99]\tvalid_0's l2: 0.211835\n",
      "[100]\tvalid_0's l2: 0.211697\n"
     ]
    },
    {
     "data": {
      "text/plain": "LGBMRegressor(max_depth=10, num_leaves=1000, random_state=42)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_regr = lgb.LGBMRegressor(num_leaves=1000,\n",
    "                              max_depth=10,\n",
    "                              random_state=42)\n",
    "\n",
    "lgbm_regr.fit(X_train,y_train,verbose=True,eval_set=(X_test,y_test),eval_metric='rmsle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSLE LGBM Decision tree 0.19593781047373218\n"
     ]
    }
   ],
   "source": [
    "preds = lgbm_regr.predict(X_test)\n",
    "\n",
    "drop_indeces=np.where(preds<0)\n",
    "preds=np.delete(preds,drop_indeces)\n",
    "y_test_l=np.delete(y_test.values,drop_indeces)\n",
    "\n",
    "print('Validation RMSLE LGBM Decision tree', mean_squared_log_error(y_test_l,preds, squared=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}